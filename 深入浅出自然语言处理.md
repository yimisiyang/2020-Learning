# 深入浅出自然语言处理（NLP）

## 0.概述

**课程门槛：**

1.具有一定的Python基础

2.需要一点算法基础

3.使用过wxPython、Django 更佳

**使用工具**

PyCharm 和 Navicat

**课程项目**

1.文章摘要器

2.最相关文章搜索引擎

## 1. NLP介绍

**NLP-自然语言处理**

NLP( Natural Language Processing）自然语言处理，用计算机来处理、理解以及运用人类语言，让人与计算机之间可以使用自然语言进行有效的通信。

**NLP-自然语言处理定义**
 NLP(Natural Language Processing）自然语言处理，用计算机来处理、理解以及运用人类语言，让人与计算机之间可以使用自然语言进行有效的通信。处理自然语言的关键是要让计算机“理解”自然语言，所以自然语言处理又叫做自然语言理解（NLU, Naturallanguage Understanding),也称为计算语言学（ Computational Linguistics）。一方面它是语言信息处理的一个分支，另ー方面它是人工智能（AI, Artificial Intelligence）的核心课题之一。

**NLP与AI**
 人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门技术科学。其定义就是让机器实现原来只有人类才能完成的任务。 NLP是AI的一个子方向，NLP也是A中最为困难的问题之一。当计算机具备了处理自然语言的能力时，机器才算实现了真正的智能。

**NLP研究范围**

![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200424160535.png)

**NLP 研究难点**

消除自然语言中的歧义（消歧）

**NLP 作用**

垃圾邮件识别、机器翻译、中文输入法、推荐系统、聊天系统、搜索引擎。

## 2. NLP的流程

**NLP相关名词解释**

- 语言层

  ![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/92c74a2d81e9bdb3972b32c2f43d058.png)

- 抽取层

  ![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200424162320.png)

- 挖掘层

  ![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200424162427.png)

- 应用层

  ![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200424162622.png)

- 其它相关名词

  语料库：整理过的语言文本，将真实语言中出现的一些词汇、语言材料整理起来，语料库是很多分词工具的根基。
  语言模型：其实就是算法，一般用于处理结构化后的语言，语言模型在NLP中占有重要的位置。

  词向量：将词抽象成多维空间中的向量，方便语言模型使用。

  词性标注：为自然语言文本中的每个词汇斌予一个词性的过程。

## 3. 常用的分词工具

### 1. 中文分词概述

- 与英文为代表的拉丁语系语言相比，英文以空格作为天然的分隔符，而中文由于继承自古代汉语的传统，词语之间没有分隔，所以需要对中文进行分词。
- 中文分词的方法其实不局限于中文应用，也被应用到英文处理，如手写识别，单词之间的空格就不很清楚，中文分词方法可以帮助判別英文单词的边界。
- 中文分词是中文自然语言处理的基础技术，分词的准确性直接影响到NLP应用的好坏。

### 2. 中文分词基本算法

- 字符串匹配（机械分词法）
  按照一定的策略将待分析的字符串与一个充分大的词典中的词条进行匹配。若在词典中找到某个字符串，则匹配成功，将匹配结果切分出来。
- 基于统计以及机器学习的分词方法
  基于人工标注的词性和统计特征，对中文进行建模。在分词阶段再通过模型计算各种分词出现的概率。将概率最大的分词结果作为最终结果。
- 计算机模拟人对句子的理解（试验阶段）
  在分词的同时进行句法、语乂分析，利用句法信息和语义信息来处理歧义现象。

### 3. 基本算法

**正向最大匹配（MM)**

- 从左向右取待切分汉语句的m个字符作为匹配字段，m为大机器词典中最长词条个数。
- 查找词典并进行匹配。若匹配成功，则将这个匹配字段作为ー个词切分出来，若匹配不成功，则将这个匹配字段的最后一个字去掉，剩下的字符串作为新的匹配字段，进行再次匹配。
-  重复执行以上过程，直到切分出所有词为止。

**逆向最大匹配（RMM）**

- 从右向左取待切分汉语句的m个字符作为匹配字段，m为大机器词典中最长词条个数。
- 查找词典并进行匹配。若匹配成功，则将这个匹配字段作为ー个词切分出来，若匹配不成功，则将这个匹配字段的最前一个字去掉，剩下的字符串作为新的匹配字段，进行再次匹配。
-  重复执行以上过程，直到切分出所有词为止。

**双向最大匹配法（广泛使用）**

将正向最大匹配法得到的分词结果和逆向最大匹配法的到的结果进行比较，从而获得最终的分词结果。

-  中文中90.0%左右的句子，正向最大匹配法和逆向最大匹配法完全重合且正确。
- 大概9.0%的句子两种切分方法得到的结果不一样，但其中必有一个是正确的。
-  不到1.0%的句子，匹配不出正确结果。

**基于n元语法的分词算法**

在前n-1个词出现的条件下，下一个词出现的概率是有统计规律的，出现的概率可以基于大量语料统计得出（中文分词的统计学基础）。

- 根据词典将一句话中所有可能的分词匹配出来。

- 将匹配出来的词和该句中的每个字作为节点，构建有向分词图。

-  分词图边的权重为节点词的概率。

- 利用相关搜索算法（动态规划）从分词图中找到权重最大的路径作为最后的分词结果。

  ![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200424165255.png)



**隐马尔可夫模型（HMM）**

- 隐马尔可夫模型是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。
-  隐马尔可夫模型可以从可观察的参数中确定该过程的隐含参数，然后利用这些参数来作进一步的分析。
-  经常用作能够发现新词的算法，通过海量的数据学习，能够将人名、地名、互联网上的新词识别出来。

解决问题

- 估计问题

-  参数估计问题：分词的学习阶段，通过海量的语料数据来学习归纳出分词模型的各个参数
-  状态序列可题：分词的执行阶段，通过观察变量（即待分词句子的序列）来预测出最优的状态序列（分词结构）

**常见的中文分词工具**

![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200424165853.png)

**中文分词工具有很多，每个分词工具都有不同的特点，本门课将使用 Hanlp分词工具和结巴分词工具。**

## 4. HanLP介绍

### 1. HanLP 介绍

- Hanlp是由一系列模型与算法组成的Java工具包，目标是普及自然语言处理在生产环境中的应用。 Hanlp具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。
-  Hanlp实现了中文分词、词性标注、关键词提取、自动摘要、短语转換、拼音转換、文本推荐、依存句法分析等功能

### 2. HanLP 简单使用

下载HanLP Protable版 [下载网址]( http://t.cn/RfMdLJf)(因为HanLP是用Java写的，所以需要能够python调用java库)。

下载Jpype (用python调用java的一个库)[下载网址](https://pypi.python.org/pypi/JPypel-py3#downloads)，或者通过pip安装。

**注意：** 

- 使用Jpype调用Java时，要保证python位数、Jpype位数、Java位数一致（如都为64位）。

- 使用pip安装Jpype时，要注意python2和python3安装命令不太一样，python2安装命令如下：

  ```
  pip install Jpype1
  ```

Jpype使用流程：

```
#coding:utf-8
from Jpype import *
# 获取JVM路径
path = getDefaultJVMPath()

# 开启JVM(path)
startJVM(path)

# java代码
java.long.System.out.println("hello NLP")

# 关闭JVM
shutdownJVM()
```

**注意：** Jpype不兼容win10,通过修改源码解决（别人已修改好），直接下载下来用就可以，但是需要安装一个numpy包。



## 5. 实现文章摘要器和最相关文章搜索引擎

### 1.文章摘要器的成果展示和实现思路

成果展示

![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/a639b25afffb86d81307452c9588859.png)

实现思路：

WxPython实现界面布局，为两个按钮添加相应的点击事件

通过文件后缀来判断需处理文件类型

PDFMiner --->PDF (读PDF内容)

Docx2txt --->docx(读word内容)

读取以后调用Jpype调用HanLP里面的函数对文章进行分析。

### 2. 对文章摘要器进行开发

#### 1.配置开发环境

- 安装 virtualenv，创建虚拟环境（不污染系统python环境）

  ![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200426104645.png)

- 进入python虚拟环境

  ```shell
  D:\PythonEnvs> cd D:\PythonEnvs\aac\Scripts 
  D:\PythonEnvs\aac\Scripts>activate
  ```

- 安装所需pip包（python调用Java库用）

  主要是安装Jpype包和numpy包

- 安装WxPython(推荐下载后pip安装)

  下载网址： http://www.lfd.uci.edu/~gohlke/pythonlibs/

  **注意:** 安装第三方库时选32位还是64位，取决于电脑上的python位数。

- 安装pdfminer和docx2txt(读pdf和word用)

  ```python
  pip install pdfminer
  pip install docx2txt
  ```

#### 2.HanLP配置

**解释：** 这里没有使用protable版的HanLP，使用需要配置的HanLP可以有更细力度的控制。

github上下载配置版的HanLP:https://github.com/hankcs/HanLP

**注意：** 我下载时已经有了python版的HanLP，因此已经不需要python调用Java库了。

### 3.HanLP调用

编写useHanLP.py 实现python调用Java包。

**解释：**截图代码不全，最新HanLP已经有python版本了。

![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200426112218.png)

### 4.处理word和pdf文件

- 处理word

  编写DocxToStr.py文件

  ```python
  #coding:utf-8
  
  from _future_ import unicode_literals
  import docx2txt
  import useHanLP as HanLP
  def WToT(filepath):
      if filepath.split('.')[-1] =='docx' or filepath.split('.')[-1] == 'doc':
          text = docx2txt.process(filepath)
          # 这里编写处理语句
          res1 = HanLP.GetKeyword(text, 5)
          res2 = HanLP.GetSummary(text, 5)
          res3 = HanLP.GetPhrase(text, 5)
          # 创建一个list 方便返回。
          res = list()
          res.append(res1)
          res.append(res2)
          res.append(res3)
          return res
  ```

- 处理pdf

  编写PDFToStr.py文件

  ```python
  #coding:utf-8
  
  from _future_ import unicode_literals
  from cStringIO import String IO
  from pdfminer.pdfinterp import PDFResourceManager,PDFPageInterpreter
  from pdfminer.layout import LAParams
  from pdfminer.converter import TextConverter
  from pdfminer.pdfpage import PDFPage
  import useHanLP as HanLP
  def PDFToT(fname,pages=None):
      if not pages:
          pagenums = set()   
      else:
          pagenums = set(pages)
      output = StringIO()
      # 创建PDF资源管理器
      manager = PDFResourceManager()
      # 创建PDF设备对象
      laParams = LAParams()
      # 文本转换器
      converter = TextConverter(manager, output, laparams=laParams)
      # PDF解释器
      interpreter = PDFPageInterpreter(manager,converter)
      
      infile = file(fname,'rb')
      
      for page in PDFPage.get_pages(infile, pagenums):
          interpreter.process_page(page)
      infile.close()
      converter.close()
      text = output.getvalue()
      output.close()
      return text
  
  def GetPDF(pdfpath):
      if pdfpath.split('.')[-1] == 'PDF' or pdfpath.split('.')[-1] == 'pdf':
          text = PDFToT(pdfpath)
            # 这里编写处理语句
          res1 = HanLP.GetKeyword(text, 5)
          res2 = HanLP.GetSummary(text, 5)
          res3 = HanLP.GetPhrase(text, 5)
          # 创建一个list 方便返回。
          res = list()
          res.append(res1)
          res.append(res2)
          res.append(res3)
          return res
  
  # 代码测试    
  if _name_ == '_main_':
      pdfpath = r''   # PDF 路径
      res = GetPDF(pdfpath)
      print res
  ```

  ### 5.实现文章摘要器布局

  **提示：** 界面通过WxPython实现。

  编写wxmain.py文件

  ```python
  #coding:utf-8
  from _future_ import unicode_literals
  import wx
  import os
  import DocxToStr as d2s   #  as用来起别名，方便使用
  import PDFToStr as p2s
  import atexit
  import useHanLP as HanLP
  # 选择文件
  def OnOpen():
      dialog = wx.FileDialog(None, '选择一个文件', os.getcwd(), '', '',wx.OPEN)
  	if dialog.ShowModal == wx.ID_OK:
          filename.SetValue(dialog.GetPath())
      # 使用完之后销毁
      dialog.Destroy()
             
  # 运算
  def OnSave():
      filepath = filename.GetValue()
          if filepath == '':
              contents.SetValue('必须选择一个文件')
          else:
              fileExtension = filepath.split('.')[-1]
              if(fileExtension == 'docx' or fileExtension == 'doc'):
                  d2s.WToT(filepath)
              elif(fileExtension == 'PDF' or fileExtension == 'pdf'):
                  p2s.GetPDF(filepath)
              else:
                  contents.SetValue('必须选择PDF文件或word文件')
              
              str = '运算开始\n\n'
              for r in res:
                  for k in r:
                      str += '--------' + k + '\n'
              str = '运算结束\n\n'
              contents.SetValue(str)
  
  # 1.定义程序类对象
  app = wx.App()
  # 2.创建顶层窗口
  win = wx.Frame(None,title='文章摘要器',size=(600,400))
  
  # 3.创建面板
  bkg = wx.Panel(win)
  
  # 4.进行界面布局
  # （1）创建两个点击按钮
  loadButton = wx.Button(bkg,label = u'选择文件')
  saveButton = wx.Button(bkg,label = u'运算')
  # （2）为点击按钮设置触发事件
  loadButton.Bind(wx.EVT_BUTTON, OnOpen)
  saveButton.Bind(wx.EVT_BUTTON, OnSave)
  # （3）创建输入框(显示路径)
  filename = wx.TextCtrl(bkg)
  # （4）创建输入框(显示文章抽取结果)
  contents = wx.TextCtrl(bkg, style = wx.TE_MULTILINE)
  # (5) 将多个控件绑定在一起形成一个大的控件
  hbox = wx.BoxSizer()
  hbox.Add(filename, proportion=1, flag=wx.EXPAND)
  hbox.Add(loadButton, proportion=0, flag=wx.LEFT, border=5)
  hbox.Add(saveButton, proportion=0, flag=wx.LEFT, border=5)
  
  vbox = wx.BoxSizer(wx.VERTICAL)
  vbox.Add(hbox, proportion=0, flag=EXPAND|wx.All,border=5)
  vbox.Add(contents, proportion=1, flag=EXPAND | wx.LEFT | wx.BOTTOM | wx.RIGHT, border=5)
  
  # （6）添加到面板中
  bkg.SetSizer(vbox)
  
  # 5.显示面板
  win.Show()
  
  # 6.使界面进入死循环
  app.MainLoop()
  
  atexit.register(HanLP.ShopJVM())
  ```

## 6.TF-IDF算法

### 1.TF-IDF算法定义

- TF-IDF算法

  TF-IDF( term frequency- inverse document frequency）是一种用于信息检索与文本挖掘的常用技术。用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。

- TF-IDF的主要思想：

  如果某个词或短语在一篇文章中出现的频率很高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

  ![](C:\Users\ThinkPad\Desktop\课程学习截图\深入浅出自然语言处理\8707f5aa0e36c7bc9a29c90e70b29d4.png)

  

### 2.TF-IDF算法应用范围

应用范围：

![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200426161004.png)

### 3.TF-IDF算法原理

- TF-IDF算法的原理

  TF-IDF7是计算某个词在文件集的其中一份文件的权重（重要程度）。

  TF-IDF=TF*IDF
  TF：词频（ Term Frequency)
  IDF：逆向文件频率（ nverse Document Frequency）

**解释：** 

TF（词频）=某个词在文章中出现的次数

考虑文章有长短之分
 TF（词频）=某个词在文章中出现的次数/文章中总词数
 IDF（逆文档频率）=文档总数n与词w所出现文件数比值的对数 = log（文件集中文档的总数数/包含该词的文档数+1)

**分母要加1,是为了避免分母为0(即所有文档都不包含该词）**

### 4.TF-IDFpython代码实现

创建tfidf.py

通过python连接MySQL数据库，首先需要安装MySQL-python库。

**windows下直接pip安装可能会出错，首先还是先下载下来在使用pip安装**

```python
#coding:utf-8
import _future_ import unicode_literals
import _future_ import division
import useMySQL
import os
import hashlib
import DocxToStr as d2s
import Json
import math

TABLE = 'filetable'
conn = useMySQL.connectMySQL()
# 编写tfidf算法
def tfidf():
    #关键词
   	keylist = ['python',u'网络',u'编码',u'极客']  #u的作用是把中文转成utf 编码，防止乱码。
    # 关键词出现的文章
	reslist =[]
	for key in keylist:
        l = useMySQL.queryMySQLnum(conn,TABLE,key)
        reslist.append(l)
    
    # tfidf
    keydict = dict()
    for l in reslist:
        for ll in l:
            keydict[ll] = 0
    # 获得文档库中文档总数
    allFileNum = useMySQL.queryALLFileNum(conn,TABLE)
    i = 0
    for idL in reslist:
        fileNum = len(idL) + 1
    	idf = math.log(allFileNum/fileNum)
        
        tf = 0
        for id in idL:
			fenres = int(useMySQL.queryMySQL(conn,TABLE, id)[0][0])
			dison = json.loads(fenres[0][0])
            allwordNum = len(djson)
            wordNum=djson.count(keylist[i])
            tf = wordNum/allwordNum
            tfidf = tf*idf
            keydict[id] += tf*idf
    	i = i+1
    print keydict
# 获取每篇文章的分词
def queryMySQLFen(conn,table,id):
    cur = conn.cursor()
    sql = "select filefen from" + table + " where id = %d" %(id)
    cur.execute(sql)
    return cur.fetchall
# 获取sql中的文章数
def queryAllFileNum(conn, table):
    cur = conn.cursor()
    sql = "select count(id) from " + table
    cur.execute(sql)
    return cur.fetchall()
# 查表方法(通过模糊匹配得到关键词出现过的文章)
def queryMySQLnum(conn, table, keyword):
    cur = conn.cursor()
    sql = "select id from " + table + " where filefen like '%" + keyword + "%'"
    cur.execute(sql)
    cur.close()
    restuple = cur.fetchall()
    reslist = list()
    for rest in restuple:
        reslist.append(int(rest[0]))
    return reslist
# 初始化文件
def initFile():
    path = r'F:\NLP\AAC\FileFata'
    for filename in os.listdir(path):
        filepath = path + '/' + filename
        name = filename.split('.')[0]
        with open(filepath,'rb') as f:
            m = hashlib.md5()
            m.update(f.read())
            # 文件MD5
            filemd5 = m.hexdigest()
        if filemd5 != 0:
            useMySQL.queryMySQL1(conn,TABLE,filemd5)
            
        if res is None:
            fen = ''
            fen = d2s.WToT(filepath)
            fenJson = json.dumps(fen, ensure_ascii= False)
            useMySQL.insertMySQL(conn,TABLE,name,filepath,filemd5,fenJson)
if _name_ = '_main_':
    initFile()
    tfidf()
```

创建useMySQL.py

```python
#coding:utf-8
import MySQLdb

# 连接数据库
def connectMySQL:
	return MySQLdb.connect('localhost','user'，'password','database_name',charset =  'utf-8')

# 判断filemd5是否存在
def queryMySQL1(conn,table,filemd5):
    cur = conn.cursor()
    sql = 'select id from ' + table + ' where filemd5 = "%s"' % (filemd5)
    cur.execute(sql)
    
    return cur.fetchone()


# 存入数据库的方法

def insertMySQL(conn, table, filename, filepath, filemd5, filefen):
    cur = conn.cursor()
    sql = 'insert into ' + table +' (filename,filepath,filemd5,filefen) values (%s, %s, %s, %s)'
    cur.execute(sql,(filename, filepath, filemd5, filefen))
    conn.commit()
    cur.close()
```

创建DocxToStr.py

```python
#coding:utf-8

from _future_ import unicode_literals
import docx2txt
import useHanLP as HanLP
def WToT(filepath):
    if filepath.split('.')[-1] =='docx' or filepath.split('.')[-1] == 'doc':
        text = docx2txt.process(filepath)
        fenlist = list()
        # 这里编写处理语句
        res = HanLP.GetFenword(text)
        for fen in res:
            fenlist.append(fen.toString())
        return fenlist
```



## 7.LDA算法

### 1.LDA算法的定义

- LDA算法的定义

   隐式狄利克雷分布（ Latent Dirichlet Allocation，简称LDA）是一种概率主题模型。

  LDA是2003年提出的一种主题模型，它可以将文档集中每篇文档的主题以概率分布的形式给出。LDA通过分析一些文档，可以从中抽取出它们的主题分布，根据主题分布进行主题聚类或文本分类。

- LDA算法的特点

  LDA是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有先后顺序的关系。一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。

- LDA的作用

  将文档集中每篇文档的主题以概率分布的形式给出。

### 2. LDA算法应用范围

LDA应用范围

![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200427093305.png)



### 3.LDA算法的原理

- 文章是如何生成的？
  1.随机从主题集中选择一个主题
  2.随机从该主题中选择一个词
  3.一直循环前面两个步骤，形成一片文章

![](C:\Users\ThinkPad\Desktop\课程学习截图\深入浅出自然语言处理\20200427093837.png)

- LDA算法原理

  LDA就是将上面的过程反转了，它计算出选择某个主题的概率和选择该主题下某个词的概率，这两个概率分布计算清楚，就可以建立主题模型。

  LDA的生成过程对应的观测变量和隐藏变量的联合分布公式：

  ![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200427094533.png)

### 4.LDA算法python实现（使用第三方库）

[gensim库](https://radimrehurek.com/gensim/index.html)实现了LDA算法。

[python第三方库网站](www.lfd.uci.edu/~gohlke/pythonlibs/)

**注意：** gensim 需要scipy包的支持。按装gensim时先安装scipy

编写lda.py

```python
#coding:utf-8
from _future_ import unicode_literals
import docx2txt
import gensim
doc_path = r'D:\自然语言处理.docx'
# 将每一句当成一篇独立的文章
doc_list[] = docx2txt.process(doc_path).split('\n')
texts = []
for doc in doc_list:
    text = HanLP.GetFenword(doc)
	text_list = list()
    for t in text:
        text_list.append(t.toString())
    texts.append(text_list)
# 为不重复的单词设置一个ID，并记录出现次数    
dictionary = gensim.corpora.Dictionary(texts) 
# 词袋(文档词频矩阵)
corpus = [dictionary.doc2bow(text) for text in texts]

ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics = 2,id2word = dictionary, passes = 20)

print ldamodel.print_topics(num_topics=2, num_words = 3)
```

## 8. jieba的使用及算法详解

### 1. jieba分词工具的介绍

- jieba分词

  目标与愿景：做最好的 Python中文分词组件

  普及度：有很多不同语言版本的 jieba,R语言、GO语言、.NET版本等。

- 特点

  支持三种分词模式（精准模式、全模式、搜索引擎模式）

  支持繁体分词

  支持自定义词典

- pip直接安装

  ```
  pip install jieba
  ```

  

### 2. jieba分词工具的使用

- 精确模式分词

  ```
  jieba.cut("欢迎学习深入浅出NLP")
  ```

- 全模式

  ```
  jieba.cut("欢迎学习深入浅出NLP",cut_all=True)
  ```

- 搜索引擎模式

  ```
  jieba.cut_for_search("欢迎学习深入浅出NLP")
  ```

- 载入自定义字典（别用记事本创建字典，报编码错误）

  ```
  jieba.load_userdict(file_name)
  ```

- 动态修改字典

  ```
  jieba.add_word('石墨烯')
  jieba.del_word('石墨烯')
  ```

### 3.TextRank 算法原理

- Page Rank的原理

   Pagerank是一种通过网页之间的超链接来计算网页重要性的技术。

- 整个互联网可以看作是一张有向图，网页是图中的节点，网页之间的链接就是图中的边。

- 如果网页A存在到网页B的链接，那么就有一条从网页A指向网页B的有向边。

![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/c6dd9d4b7deaea12e734b53615a88f4.png)

![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/01d293d59006eb3fef1b02cc210a3a8.png)

S(Vi）是网页的中重要性（PR值）

d是阻尼系数，一般设置为0.85

ln(Vi）是在指向网页的链接的网页集合

Out(Vj）是网页j中的链接存在链接指向的网页集合

|Out(Vj)| 是集合中元素的个数

- TextRank原理

  Textrank公式在 Pagerank公式的基础上，为图中的边引入了权值的概念。

  ![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200427151007.png)

Wij 就是图中节点Vi到Vj的边的权值

d依旧是阻尼系数，一般取值0.85

In(Vi)和Out(Vj)分别是指向Vi节点的集合和从Vj节点出发的边指向的节点集合。

- TextRank算法步骤

  （1）把文本拆分为单词，过滤掉停用词，保留指定词性的单词，得到了单词的集合。如: W1，W2 W3，W4，W5，W6.... Wn

  （2）设定窗口大小为k 。[w1,w2,wk]、[w2,w3,...,wk+1]、[w3,w4,....,k+2]都是一个窗口。

  （3）每个单词作为图中的一个节点，同一个窗口中的任意两个单词对应的节点之间存在着一条边。

  （4）利用投票的原理，将边看成是单词之间的互相投票。

  （5）通过矩阵迭代收敛，每个单词的得票数都会趋于稳定。

  （6）个单词的得票数越多，就认为这个单词越重要。

- TextRank算法Python实现

  [Text Rank4ZH](https://github.com/letiantian/TextRank4ZH)

### 4. jieba 去除停用词

不用词的列表。

## 9.开发文章搜索引擎

### 1.成果展示

成果展示图（类似于浏览器搜索）

前端搜索界面

![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200427155153.png)

后端管理界面

![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200427155606.png)



### 2.思路梳理

- 具体实现思路

  （1）搭建并配置Django开发环境

  （2）前端界面的编写和数据库的设计
  （3）编写TF-IDF算法，将与用户输入关键词最相关的5篇文章返回
  （4）编写LDA算法，对返回的内容做主题概率运算
  （5）使用 xadmin完成后台

### 3.文章搜索引擎实现

#### 1.Diango介绍

Django是由 Python写成一个开放源代码的Web应用框架，采用了MVC的软件设计模式，使得开发复杂的、数据库驱动的网站变得简单。

Django框架的核心包括：
（1）一个面向对象的映射器，用作数据模型和关系性数据库间的媒介

（2）一个基于正则表达式的URL分发器

（3）一个视图系统，用于处理请求

（4）一个模板系统

#### 2.搭建开发环境

- 安装Diango	

```pip
pip install Diango==1.9.0
```

- pyCharm创建Diango项目

  配置要连接的数据库信息

- 在Navicat上创建相关的数据库

- 在pyCharm中初始化Diango项目，Diango会自动在数据库中创建一些项目运行所需要的表。


#### 3.数据库设计与前端整合

编写创建的app中的model.py 设计数据库，这里就不用写底层sql语句了

![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200427163357.png)

每次修改完都需要运行项目执行以下两条命令

![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200427163554.png)

以上数据库就配置完了，下面看前端界面。

- 将前端界面代码复制到项目中

  编辑创建项目中的 views.py 实现前端映射。

  ![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200427164412.png)

- 编辑url.py

  ![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200427164602.png)

- 此时界面没有效果，需要告诉Django怎么找css文件。主要是更改项目中的setting.py文件

![image-20200427165001453](C:\Users\ThinkPad\AppData\Roaming\Typora\typora-user-images\image-20200427165001453.png)

![](https://raw.githubusercontent.com/yimisiyang/cloudimage/master/Image/20200427165130.png)

#### 4.TF-IDF算法整合

和前面TF-IDA算法实现类似

#### 5.LDA算法整合(部分)

编写lda.py

```python
#coding:utf-8
import gensim 
import json
from article.models import ArticleModel

# reslist 文章id的list
def getTheme(reslist):
    fencelist = list()
    for res in reslist:
        id = res[0]
        article = ArticleModel.objects.get(id =id)
        filefence = json.loads(articel.file_fence)
        fencelist.append(filefence)
    dictionary = gensim.corpora.Dictionary(fencelist)
    # 文档词频矩阵
    corpus = [dictionary.doc2bow(word) for word in fencelist]
    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=3,id2word=dictionary,passes=20)
    return ldamodel.print_topics(num_topics,num_words=3)
```

#### 6.使用xadmin

- admin介绍
  admin是由 python编写的基于 Django的后台管理框架，使用 admin只需定义您数据的字段等信息，就可获得一个功能全面的管理系统。

- xadmin的特点

  （1）基于Bootstrap3，支持多主题，支持多屏幕。

  （2）xadmin是款全面的后台系统框架，提供很多功能

  （3）拥有强大的插件系统

- 安装xadmin

  ```
  pip install xadmin
  ```

  